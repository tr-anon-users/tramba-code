{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b596982",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, pickle, random\n",
    "import numpy as np, pandas as pd, torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from data_loader import load_data\n",
    "from models import Tramba\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b24d119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ── Configurations ──────────────────────────────────────────\n",
    "SEQ_LIST   = [36]               \n",
    "PRED_LIST  = [1, 6, 12, 24, 36]\n",
    "N_FEATURE  = 3\n",
    "D_MODEL    = 16\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS     = 10\n",
    "\n",
    "DATA_PATH  = \"data/seoul_traffic_speed.csv\"\n",
    "LINK_PATH  = \"data/GN_links.csv\"\n",
    "BASE_DIR   = \"results\"\n",
    "os.makedirs(BASE_DIR, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ca0a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tramba(model, tr_loader, vl_loader, label, scaler, save_dir):\n",
    "    model.to(device)\n",
    "    opt  = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    crit = torch.nn.MSELoss()\n",
    "    hist = {\"train\": [], \"val\": []}\n",
    "\n",
    "    for ep in range(1, EPOCHS + 1):\n",
    "        model.train(); tr_l = []\n",
    "        for x, y in tqdm(tr_loader, desc=f\"{label} {ep}/{EPOCHS}\", leave=False, ncols=80):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            opt.zero_grad(); loss = crit(model(x), y); loss.backward(); opt.step()\n",
    "            tr_l.append(loss.item())\n",
    "        model.eval(); vl_l = []\n",
    "        with torch.no_grad():\n",
    "            for x, y in vl_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                vl_l.append(crit(model(x), y).item())\n",
    "        hist[\"train\"].append(np.mean(tr_l))\n",
    "        hist[\"val\"].append(np.mean(vl_l))\n",
    "\n",
    "    with open(os.path.join(save_dir, f\"{label}_hist.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(hist, f)\n",
    "\n",
    "    preds, trues = [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in vl_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            preds.append(model(x).cpu().numpy()); trues.append(y.cpu().numpy())\n",
    "            \n",
    "    preds, trues = np.concatenate(preds), np.concatenate(trues)\n",
    "\n",
    "    if preds.ndim == 4:\n",
    "        preds, trues = preds[:, -1, :, 0], trues[:, -1, :, 0]  # shape (B, L)\n",
    "    else:\n",
    "        preds, trues = preds[:, -1, :], trues[:, -1, :]        # shape (B, L)\n",
    "\n",
    "    pr = scaler.inverse_transform(preds.reshape(-1, preds.shape[-1])).reshape(preds.shape)\n",
    "    tr = scaler.inverse_transform(trues.reshape(-1, trues.shape[-1])).reshape(trues.shape)\n",
    "\n",
    "    mask = np.abs(tr) > 1e-3\n",
    "\n",
    "    return dict(\n",
    "        MAPE = round(np.mean(np.abs((tr[mask] - pr[mask]) / tr[mask])) * 100, 2),\n",
    "        MAE  = round(np.mean(np.abs(tr - pr)), 3),\n",
    "        MSE  = round(np.mean((tr - pr) ** 2), 3),\n",
    "    ), model\n",
    "\n",
    "\n",
    "# ── Run for SEQ_LEN × PRED_LEN combinations ────────────────\n",
    "summary = []\n",
    "for SEQ_LEN in SEQ_LIST:\n",
    "    SAVE_DIR = os.path.join(BASE_DIR, f\"s{SEQ_LEN}\")\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "    for P in PRED_LIST:\n",
    "        print(f\"\\n=== Tramba | seq_len {SEQ_LEN} | pred_len {P} ===\")\n",
    "\n",
    "        tr_loader, vl_loader, scaler = load_data(\n",
    "            DATA_PATH, LINK_PATH, seq_len=SEQ_LEN, pred_len=P, batch_size=BATCH_SIZE)\n",
    "\n",
    "        label   = f\"Tramba_p{P}_s{SEQ_LEN}\"\n",
    "        wt_path = os.path.join(SAVE_DIR, f\"{label}_wt.pth\")\n",
    "\n",
    "        if os.path.exists(wt_path):\n",
    "            print(f\"{label} exists, skipping.\")\n",
    "            continue\n",
    "\n",
    "        model = Tramba(d_model=D_MODEL, in_features=N_FEATURE, pred_len=P)\n",
    "\n",
    "        t0 = time.time()\n",
    "        try:\n",
    "            metrics, trained = train_tramba(model, tr_loader, vl_loader, label, scaler, SAVE_DIR)\n",
    "            runtime = round(time.time() - t0, 1)\n",
    "\n",
    "            torch.save(trained.state_dict(), wt_path)\n",
    "            with open(os.path.join(SAVE_DIR, f\"{label}_runtime.txt\"), \"w\") as fp:\n",
    "                fp.write(str(runtime))\n",
    "\n",
    "            metrics.update(Model=\"Tramba\", pred_len=P, seq_len=SEQ_LEN, Time_s=runtime)\n",
    "            summary.append(metrics)\n",
    "\n",
    "            print(f\"{label} done ({runtime}s)\")\n",
    "        except Exception as e:\n",
    "            print(label, e)\n",
    "\n",
    "# ── Save Summary Table ──────────────────────────────────────\n",
    "csv_path = os.path.join(BASE_DIR, \"results_tramba.csv\")\n",
    "if summary:\n",
    "    df = pd.DataFrame(summary)\n",
    "    if os.path.exists(csv_path):\n",
    "        df_all = pd.concat([pd.read_csv(csv_path), df], ignore_index=True)\n",
    "        df_all = df_all.drop_duplicates(subset=[\"Model\", \"pred_len\", \"seq_len\"])\n",
    "    else:\n",
    "        df_all = df\n",
    "    df_all.to_csv(csv_path, index=False)\n",
    "    display(df_all)\n",
    "    print(\"saved to\", csv_path)\n",
    "else:\n",
    "    print(\"No new runs.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dgm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
